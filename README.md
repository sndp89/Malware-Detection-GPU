# Featureless LSTM-for-Malware-detection-using-GPU

## Problem Statement

TPX (Technology, Product, Experience) has reached out for assistance in detecting malicious domains, which are created by domain generation algorithms (DGA).

Domains (e.g. gmail.com, facebook.com) are fundamental to the internet. They provide a hierarchy of unique identifiers that guide traffic across the web and identify websites, servers and other resources. On the hand, when malicious, they are powerful tools in the hands of cybercriminals.

A simple binary classification exercise that identifies a domain as either DGA or benign can arm Comcast against potential malware infections.

## Understanding Malware Infection

       {Phase 1: Infection}  --------->   {Phase 2: Callback}
       
   ### Phase 1: Infection
 
First phase of malware infection consists of System Exploit and Binary Loading. A System Exploit is a malicious material (e.g. a code or software) taking advantage of a bug, glitch, or vulnerability in a service, host, server, network, or a more complicated system to perform unauthorized access, illegal privilege escalation, data reveal, or denial-of-service against the system.

A System Exploit can happen with or without end user engagement. In some cases, such as a phishing email or web link, attackers tempt the end user to click on URL links in order to execute some malicious material to achieve the exploit . In other cases, attackers can directly exploit the system with sophisticated code or data that targets vulnerabilities.

In Binary Loading, after the system is exploited, the browser downloads a malware binary, a "dropper," generally fetching it from a website completely independent of the original exploit website. Using a separate site to host the dropper helps hide the exploit source. The exploited browser then unpacks and executes the binary to load the attackers’ full malware toolkit. When the toolkit is loaded, the malware binary is ready to communicate with the Command and Control (C&C) host.

   ### Phase 2: Callback
   
Second phase consists of Callback and Data Exfiltration. Malware callbacks normally travel from the internal network to external hosts. The malware binary instructs the infected machine to transmit network callback traffic to the C&C host to signal the attacker that it is ready to be controlled remotely.

However, some malware callbacks originate from external hosts and go to the internal network. In this infrequent scenario, external hosts scan the internet to find infected machines. In this case, the C&C host scan can randomly be found in the traffic that tries to enter the internal network.

When the callback has established a connection, the C&C host can now control the infected machine, collect data, and transmit the data back to C&C host or another destination. After that, a “worm” transmits the copies of malware to new victims, as known as Data Exfiltration. The worm can send out any information from the “targeted victim” host.

Below is an example scenario of a typical malware infection:


      ### Example
  
      A email with a URL link is received by a user; the user clicks the URL, not realizing 
      it's malicious. 
      The link takes the user to an innocent-looking, possibly even popular and well-known, 
      website that executes malicious JavaScript.

      System Exploit -----> The browser is redirected to a controlled website that executes malicious 
      JavaScript - 
      for example, 
      containing the heap spray attack code and corresponding shellcode - and compromises the browser.

      Binary Loading -----> The exploited browser, now partially or fully controlled, 
      runs the shellcode 
      and downloads a keylogger Trojan archive file from the C&C host. 
      Later the Trojan is unpacked and installed in the infected machine by the shellcode.

      Callback -----> The installed keylogger Trojan does not do malicious activity right after 
      the installation; 
      it sleeps for several days. 
      After that, it opens a network connection to a C&C host to collect the data from the victim.

      Data Exfiltration -----> When the end user types on the keyboard, 
      the Trojan transmits the characters to the C&C server. 
      In this case, all the data that the end user types will be exported to the C&C server.
      
      
## Solution Proposal

Multiple types of malware use DGAs to generate a large number of pseudo-random domain names. In order to block the traffic from those malicious domains, security organizations must first discover the algorithm by reverse engineering malware samples. This process is tedious. 

An alternative approach is to detect DGA algorithm on a per domain basis with no information available except for the domain name. The domain names generated by DGAs can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, because the infected computers will attempt to contact some of these domain names every day to receive updates or commands. The use of public-key cryptography in malware code makes it unfeasible for law enforcement and other actors to mimic commands from the malware controllers as some worms will automatically reject any updates not signed by the malware controllers.

That is why we present a DGA classifier, which leverages Long Short Term Memory (LSTM) networks for real-time prediction of DGAs without the need for contextual information or manually created features.
This featureless real-time technique using LSTMs to classify DGAs has four advantages over others:


Using LSTMs to classify DGAs has four advantages over other techniques:

   1) The LSTM DGA is featureless, meaning that it operates on raw domain names (e.g. facebook.com, google.com, etc.). If a new family of DGA appears, then the classifier can be retrained without the tedious step of hand picking features. LSTMs work largely as a black box making it very difficult for adversaries to reverse engineer and beat a classifier without the same training set.
 
   2) This technique has a significantly better true positive rate/false positive rate over previously published, retrospective or real-time, approaches.
    
   3) This technique works in a multiclass classification setting. Therefore, the algorithm not only provides a binary decision of whether a domain is DGA, but also accurately fingerprints the DGA’s unique structure.
    
   4) This algorithm can classify in real-time using absolutely no contextual information. The technique is trivial to implement. It can run on virtually any security environment. The model accepts variable-length character sequences as input, so that there is no auxiliary requirement for feature extraction


# Starting below are the technical details of the project


            
  
## Deep Learning Framework -- Why LSTM ?

Before getting into details about LSTM , lets try to understand basics about RNNs(Recurrent Neural Networks).

Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.

One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.

Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. 

##### The diagram below shows the long term dependency issue in RNN. The things said by the Xs at the start of the sequence are not remembered by A at the end.

 ![RNN- long term dependency issue](https://cstwiki.wtb.tue.nl/images/RNN-longtermdependencies.png)


Imagine a messenger taking information from the first cell to the last. It first “listens” to X0 and then listens to X1 and so on till it reaches the last state:

     X0: “Hey A! *Important info*”
      A: “Okay. Got it.”
     X1: “Hey A! *Irrelevant info*”
      A: “Sure.”
     X2: “Hey A! *Important info*”
      A: “Okay.”


But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.
     
 
 
#### Long Short Term Memory networks – usually just called “LSTMs” – are special kinds of of Recurrent Neural Networks(RNNs), capable of learning long-term dependencies. 

LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. 


 ![Long Short Term Memory module](https://i.stack.imgur.com/jN448.png)

   Imagine this as another messenger:

     X0: “Hey A! *Important info*”
      A: “Okay. Got it.” *Writes down the info*
     X1: “Hey A! *Irrelevant info*”
      A: *Frowns* “Umm… I think I’ll pretend I didn’t hear it.”
     X2: “Hey A! *Important info*”
      A: *Looks at the paper* “Hmm… This changes some part of the info X0 had shared…” 
         *Updates the paper accordingly* “Okay. Updated.”


In the LSTM diagram above, Basically there’s an information highway that goes through the network which is gated. So, instead of just adding in the information gained when listening to each X, it is more “selective” about what it chooses to forget from its memory, what it listens to and what information it adds to its (long-term) memory.

LSTM is used for detecting malicious domains as it better captures meaningful temporal relationships among letters(e.g. f,c,oo,ce,o,k) in a domain names (e.g. www.facebook.com) 

## Methodology: Deep Learning Framework

All LSTM code was ran in Python 2.7.12 version using the Keras framework. The LSTM layer can be considered as implicit feature extraction which is to extract valid characters from the domain names that classifies them as good/bad domains. LSTM learns patterns fo characters that maximize the performance of the classification layer. The model pipeline is:


       Input sequence --> embedding layer --> LSTM layer --> Dense layer --> output probability

Input sequence is the input characters (e.g. oo, gl) of the domain name (e.g. google.com) which is fed to the embedding layer to a sequence of vectors. The input domain consists of non-redundant valid domain name characters (lowercase alphanumeric, period, dash and underscore) and the output dimension d is a tunable parameter that represents the embedding. In our model, we choose by grid search method for d = 128  which means that the embedding layer learns a 128-dimensional vector representation for each character in the set of valid domain characters. 

The state of an LSTM cell has an initial value that is updated as each character of a domain is fed through the model. It is a function of the current input (embedded character vector) and the previous emission of the LSTM cell. In turn, the LSTM’s emission is a function of the current state, current input, and previous emission. Each LSTM cell acts somewhat as an optimized feature extractor on the sequences of embedded character vectors produced from the previous embedding layer, and the cell’s state provides an indication of what the cell is tracking. In our model, the final emission (corresponding to the last character in the domain) from each of 128 LSTM cells is fed to the final dense layer of the model to produce the DGA probability score ranging from 0 to 1. So this output layer can be called as logistic as it's there just to convert a 128-dimensional vector to a 1-dimensional scalar whether the domain is good/bad.

As we know, The LSTM cell’s design with multiplicative gates allows a network to store and access state over long sequences, thereby capable of learning long-term dependencies. For our use with domain names, the state space is intended to capture combinations of letters that are important to discriminating DGA domains from non-DGA domains. This flexible architecture generalizes manual feature extraction via bigrams (a pair of letters), for example, but instead learns dependencies of one or multiple characters, whether in succession or with arbitrary separation.

This architecture has also the following significant functions:
    
      1. It accepts variable-length character sequences as input, so that 
         there is no auxiliary requirement for feature extraction.
               
      2. The architecture is very compact, comprised simply of an embedding layer, an LSTM network layer, 
         and a fully connected output layer that is simple dense layer with sigmoid activation.
               
      3. Although training on a large dataset is computationally intensive, 
         the shallow structure allows for very fast query times.
         
         
## Data

  - The top 1 million real domains (published by Amazon) are used for training domains that are not DGAs. 
  - We use 11 Domain Generation Algorithms to generate random domains, just like malware would. We set the 1 million "bad" domains to be generated from each of those 11 DGAs.
  - Total training dataset will consists of approximately 12 Million rows including good and bad domains.                         

## Why batch size was increased and how does it trade-off ?

Before explaining why it was increased, let’s go through some basic definitions that were used in training big datasets:

    Epochs-  One Epoch is when an ENTIRE dataset passed forward & backward through neural network only ONCE
    Batch Size-  Total number of training examples present in a single batch.
    Number of batches-  Divide the dataset into Number of Batches or sets or parts.
    Iterations-  Iterations is the number of batches needed to complete one epoch.

    So this case, we have 2 Millions rows of dataset into batch size of 2048 , 
    which means 2M/2048 is ~1000 number of batches for each epoch. 
    We have set a max_epochs=25 until the loss functions minimization saturates.

Increasing batch size increases the runtime of the LSTM as it equates to more data parallelism in Keras. Data parallelism consists of replicating the target model once on each device, and using each replica to process a different fraction of the input data. Keras has a built-in utility, keras.utils.multi_gpu_model, which can produce a data-parallel version of any model. It achieves quasi-linear speedup on up to 8 GPUs.

Increasing batch size creates the need for more epochs to minimize loss function than with lower batch size. So there is always a batch size vs. epoch trade off.


## Model Results Interpretation

  As we know that the model produces a DGA probability score from 0 to 1, Values closer to 0 are good domains and values closer to 1 are classified as bad domains produced by dga algorithms.

 
## Model Results and Evaluation metrics

  Results are significantly better than most other techniques such as using Bigram classifier, providing 0.998 area under the curve for binary classification. In other terms, this LSTM can provide a 90% detection rate with a 1:10000 fall out (False positive rate).

LSTM network using Keras was used to predict DGAs. 12 million rows were trained, including both benign and DGA domain names. On a local machine, it took ~4.27 hours compute time for one epoch:


 
        9119931/9119931 [==============================] - 15370s 2ms/step - loss: 0.0354
 
  whereas running using the docker container on the GPU reduced compute time to ~6 mins per epoch with of course increasing the batch size to parallelize utilizing all the 4 GPU's.
  
        9119931/9119931 [==============================] - 363s 41us/step - loss: 0.0324
  
 
 
 
  Here is the last epoch (Epoch 3) that it ran up to before it saturated with minimizing loss function,
  
        9119931/9119931 [==============================] - 357s 39us/step - loss: 0.0317
        Epoch 3: auc = 0.998186 (best=0.996578)
        [[ 189670    9849]
         [  13265 2187199]]

## Challenges

* Computing infrastructure -- It required large scale dataset training to be able to increase the accuracy of predicting DGAs correctly. Both the paper and this repo use the Alexa top 1 million as benign, but this repo generates its own domains for simplicity. The code in the repo generates only 10,000 DGAs which was insufficient for balanced training per our stakeholder. Since we had 1M rows of real domains, we needed to generate 1M rows of DGAs to balance the classes. So we needed to use our NVIDIA GPU infrastructure.

* Software versions compatibility -- Python versions were initially incompatible with some python packages such as tldextract and some built-ins offered by Keras. Keras multi_gpu_model and model.save() wrappers do not work in the same versions of Keras. So, we had to figure out a way to obtain the sequential layer details from parallel model summary to obtain the model weights.

* Keras/tensorflow -- We all know Keras is built on top of tensorflow. By default, tensorflow will allocate space on all available GPUs but will not always use them all for computation. Kind of a unconventional feature of tensorflow, but it has always been that way. So, we used a Keras wrapper to perform data parallelism and compute using all our 4 GPUs which reduced the runtime by ~90%.

* DNS server issue -- Initially we were unable to install Python packages inside the container due to some internet connection issues. We resolved this issue by changing DNS settings manually, although we couldn’t identify what specifically resolved it. This approach is never encouraged - NEVER MESS AROUND WITH DNS SETTINGS!


## Conclusion

   We have presented an approach, using LSTM networks, to classify DGA generated domains. LSTMs are advantageous over other techniques since they are featureless, using raw domain names as its input. There is no need to manually create features that are difficult to maintain and can be rendered useless in an adversarial machine learning setting. In addition, an LSTM classifier can be run in real-time on single domains on standard commodity hardware making it trivial to deploy in virtually all security settings. Experiments on publicly available datasets showed that the LSTM classifier performed significantly better than other techniques (both real-time and retrospective), with the ability to classify more than 90% of DGAs with a false positive rate of 1:10000. In addition, the LSTM classifier may be trivially modified for multiclass classification, which can provide context about the origin and intent of the domain generating malware.
   
From the Comcast security team's perspective: they can now detect these DGAs from their logs (which consists of all links/emails that employees receive/click or navigate) using this LSTM binary classification technique and eventually block them or deeply re-investigate those domains.


# APPENDIX


## DGAs (Domain Generation Algorithms) used to generate training data for malicious domains :

       cryptolocker
       banjori
       corebot
       dircrypt
       kraken
       lockyv2
       pykspa
       qakbot
       ramdo
       ramnit
       simda
       

## Code Snippet of LSTM Architecture

 All code/data required to run the model is attached to this github repo. Here is a mini-snippet of LSTM using Keras and its main block, to describe what's happening:
 
 
       import dga_classifier.data as data
       import numpy as np
       from keras.preprocessing import sequence
       from keras.models import Sequential
       from keras.layers.core import Dense, Dropout, Activation
       from keras.layers.embeddings import Embedding
       from keras.layers.recurrent import LSTM
       import sklearn
       from sklearn.cross_validation import train_test_split
       from keras.utils.training_utils import multi_gpu_model
       from keras.models import load_model
       import pandas as pd
       import tldextract

       model = Sequential()
       model.add(Embedding(max_features, 128, input_length=maxlen))
       model.add(LSTM(128))
       model.add(Dropout(0.5))
       model.add(Dense(1))
       model.add(Activation('sigmoid'))
       model = multi_gpu_model(model, gpus=4)
       model.compile(loss='binary_crossentropy',
                  optimizer='rmsprop')

       X = sequence.pad_sequences(X, maxlen=maxlen)
       batch_size=1024*4                                  #multiplied by 4 as there are 4 GPU's
       model.fit(X_train, y_train, batch_size=batch_size, epochs=1)

## Model Summary 

    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    embedding_1_input (InputLayer)  (None, 63)           0                                            
    __________________________________________________________________________________________________
    lambda_1 (Lambda)               (None, 63)           0           embedding_1_input[0][0]          
    __________________________________________________________________________________________________
    lambda_2 (Lambda)               (None, 63)           0           embedding_1_input[0][0]          
    __________________________________________________________________________________________________
    lambda_3 (Lambda)               (None, 63)           0           embedding_1_input[0][0]          
    __________________________________________________________________________________________________
    lambda_4 (Lambda)               (None, 63)           0           embedding_1_input[0][0]          

    __________________________________________________________________________________________________
    sequential_1 (Sequential)       (None, 1)            136705      lambda_1[0][0]                   
                                                                     lambda_2[0][0]                   
                                                                     lambda_3[0][0]                   
                                                                     lambda_4[0][0]                   
    __________________________________________________________________________________________________
    activation_1 (Concatenate)      (None, 1)            0           sequential_1[1][0]               
                                                                     sequential_1[2][0]               
                                                                     sequential_1[3][0]               
                                                                     sequential_1[4][0]               
    ==================================================================================================
    Total params: 136,705
    Trainable params: 136,705
    Non-trainable params: 0
    __________________________________________________________________________________________________


## What docker image was used to run this code?

We used an existing docker image that was created by NVIDIA, which already had tensorflow and Python 2 version installed.

       REPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE
       nvcr.io/nvidia/tensorflow           18.06-py2           9f6a1227a54b        5 weeks ago         3.4GB
       
       
## What extra libraries/packages were installed inside the docker container?

 The file tobeinstalled.sh script is run after launching the container. That script includes all pip installs needed such as sklearn, keras, tldextract, matplotlib


## How to run this code

Download the Python scripts from this github or git clone this repo. You will notice the LSTM-for-Malware-detection folder that includes lstm_run.py script (Python script) and a dga_classifier folder that has scripts for training data generation using the 11 DGAs mentioned above in this readme write up. 

All that you need to run the code on local machine is to open linux terminal/command prompt and cd into the folder and run the lstm_run.py python code:

       cd LSTM-for-Malware-detection
       python lstm_run.py
       
If you want to run using a docker container, copy (scp command) the scripts/folders to the GPU box:

       ssh datascience@ebidgx1.cable.comcast.com
       nvidia-docker run -ti --rm --shm-size=8g --ulimit memlock=-1 -p 8888:8888 -v  /home/datascience/projects/sandeep/:/sandeep nvcr.io/nvidia/tensorflow:18.06-py2 bash
       
       sh tobeinstalled.sh
       cd /
       python sandeep/LSTM-for-Malware-detection/lstm_run.py
       
## Code Running inside a Docker container

      root@c46f5272c104:/# python sandeep/dga_predict-master/lstm_run.py 
      Using TensorFlow backend.
      /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated       in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved.       Also note that the interface of the new CV iterators are different from that of this module. This module will be     removed in 0.20.
      "This module will be removed in 0.20.", DeprecationWarning)
      fold 1/1
      Build LSTM model...
      2018-07-05 18:51:13.329917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
      name: Tesla V100-DGXS-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
      pciBusID: 0000:07:00.0
      totalMemory: 15.77GiB freeMemory: 15.33GiB
      2018-07-05 18:51:13.693398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
      name: Tesla V100-DGXS-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
      pciBusID: 0000:08:00.0
      totalMemory: 15.77GiB freeMemory: 15.35GiB
      2018-07-05 18:51:14.057768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: 
      name: Tesla V100-DGXS-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
      pciBusID: 0000:0e:00.0
      totalMemory: 15.77GiB freeMemory: 15.35GiB
      2018-07-05 18:51:14.431350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 3 with properties: 
      name: Tesla V100-DGXS-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
      pciBusID: 0000:0f:00.0
       totalMemory: 15.77GiB freeMemory: 15.35GiB
      2018-07-05 18:51:14.431429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1,       2, 3
      2018-07-05 18:51:15.285851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor       with strength 1 edge matrix:
      2018-07-05 18:51:15.285886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 
      2018-07-05 18:51:15.285897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y 
      2018-07-05 18:51:15.285904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y 
      2018-07-05 18:51:15.285911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y 
      2018-07-05 18:51:15.285918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N 
      2018-07-05 18:51:15.287258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14846 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-16GB, pci bus id: 0000:07:00.0, compute capability: 7.0)
      2018-07-05 18:51:15.476488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14864 MB memory) -> physical GPU (device: 1, name: Tesla V100-DGXS-16GB, pci bus id: 0000:08:00.0, compute capability: 7.0)
      2018-07-05 18:51:15.666444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14864 MB memory) -> physical GPU (device: 2, name: Tesla V100-DGXS-16GB, pci bus id: 0000:0e:00.0, compute capability: 7.0)
      2018-07-05 18:51:15.856365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14864 MB memory) -> physical GPU (device: 3, name: Tesla V100-DGXS-16GB, pci bus id: 0000:0f:00.0, compute capability: 7.0)
      Train...
      /sandeep/dga_predict-master/dga_classifier/lstm.py:64: UserWarning: The `nb_epoch` argument in `fit` has been renamed       `epochs`.
      model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1)
  
      9119931/9119931 [==============================] - 357s 39us/step - loss: 0.0317
      Epoch 3: auc = 0.998186 (best=0.996578)
      [[ 189670    9849]
       [  13265 2187199]]
 



# following changes made

Added a new file:  lstm_run.py 

Added model save logic in dga_classifier/lstm.py line 87:   model.save('lstm.h5')

To run just lstm model trainning:  python lstm_run.py

To test with a list of domains from Proxy Log proxy_log_04_10.csv, run Jupyter Notebook  lstm_dga.ipynb


# Predicting Domain Generation Algorithms using LSTMs
This repo contains very simple code for classifying domains as DGA or benign.  This
code demonstrates our results in our arxiv paper here: https://arxiv.org/abs/1611.00791.
One difference is the datasets.  Both the paper and this repo use 
the Alexa top 1 million as benign, but this repo generates its own domains for simplicity.

We also only implement the LSTM and bigram classifier from the paper.  These are the two best 
classifiers and are simple to implement in Keras.

## Running the code

`python run.py` will download and generate all the data, train and evaluate the classifier, and save a PNG to disk (the ROC curve). 
It defaults to 1 fold to speed things up.  This code will run on your local machine or on a machine with a GPU (GPU will of course
be much faster).

## DGA Algorithms
We have 11 DGA algorithms in our repo.  Some are from the https://github.com/baderj/domain_generation_algorithms
repo.  We noted these in each file and kept the same GNU license.  However, we made some small edits
such as allowing for no TLD and varying the size for some algorithms.  
